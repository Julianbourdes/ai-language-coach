# CLAUDE.md - AI Language Coach Developer Guide

Ce document guide les d√©veloppeurs IA (et humains) travaillant sur le projet AI Language Coach.

## Vue d'ensemble du projet

**AI Language Coach** est une application de coaching linguistique multilingue pour pratiquer l'anglais, le fran√ßais ou l'espagnol √† l'oral. L'application utilise l'IA locale (Ollama + Whisper) pour garantir la confidentialit√© totale des utilisateurs.

### Philosophie du projet

- **Local-first**: Tout fonctionne en local, aucune donn√©e n'est envoy√©e √† des serveurs externes
- **Privacy-first**: Les conversations restent sur la machine de l'utilisateur
- **Multilingual**: Support complet pour EN/FR/ES avec feedback adapt√© √† chaque langue
- **Encouraging**: L'IA est toujours encourageante, jamais condescendante
- **Progressive**: Focus sur 2-3 corrections principales plut√¥t que surcharger l'utilisateur

## Architecture

### Stack technique

```
Frontend:
- Next.js 15 (App Router)
- React 19
- TypeScript
- Tailwind CSS
- shadcn/ui components

Backend/AI:
- Ollama (llama3.1:8b) via ollama-ai-provider-v2
- Vercel AI SDK pour le streaming
- Whisper pour speech-to-text
- Zustand pour le state management

Audio:
- RecordRTC pour la capture audio
- Web Audio API
- Web Speech API pour TTS (avec s√©lection intelligente des voix)
```

### Structure du projet

```
ai-language-coach/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/                    # API Routes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transcribe/         # POST: audio ‚Üí text (Whisper)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama/             # POST: chat avec streaming
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ feedback/           # POST: analyse de texte
‚îÇ   ‚îú‚îÄ‚îÄ coach/                  # Page principale (/coach)
‚îÇ   ‚îî‚îÄ‚îÄ scenarios/              # S√©lection de sc√©narios (/scenarios)
‚îÇ
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ voice/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ voice-recorder.tsx  # Capture audio + transcription
‚îÇ   ‚îú‚îÄ‚îÄ feedback/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ highlight-text.tsx  # Affichage texte avec highlights
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ correction-tooltip.tsx  # Tooltip d√©tails correction
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ feedback-panel.tsx  # Panneau r√©capitulatif
‚îÇ   ‚îú‚îÄ‚îÄ scenarios/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scenario-card.tsx   # Card pour un sc√©nario
‚îÇ   ‚îî‚îÄ‚îÄ language-coach-chat.tsx # Interface principale
‚îÇ
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îú‚îÄ‚îÄ ollama/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.ts           # Configuration Ollama + provider
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompts.ts          # Prompts syst√®me
‚îÇ   ‚îú‚îÄ‚îÄ whisper/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.ts           # Client Whisper
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ audio-processor.ts  # Utilitaires audio
‚îÇ   ‚îî‚îÄ‚îÄ store/
‚îÇ       ‚îú‚îÄ‚îÄ conversation-store.ts  # State conversations
‚îÇ       ‚îî‚îÄ‚îÄ scenario-store.ts      # State sc√©narios
‚îÇ
‚îú‚îÄ‚îÄ types/
‚îÇ   ‚îú‚îÄ‚îÄ conversation.ts         # Message, Conversation, Stats
‚îÇ   ‚îú‚îÄ‚îÄ feedback.ts             # Feedback, FeedbackRequest/Response
‚îÇ   ‚îî‚îÄ‚îÄ scenario.ts             # Scenario
‚îÇ
‚îî‚îÄ‚îÄ public/
    ‚îî‚îÄ‚îÄ scenarios/
        ‚îî‚îÄ‚îÄ default-scenarios.json  # D√©finitions des sc√©narios
```

## Conventions de code

### TypeScript

- **Stricte**: Toujours typer explicitement les props et return types
- **Pas de `any`**: Utiliser `unknown` si vraiment n√©cessaire
- **Types centralis√©s**: Tous les types dans `/types`

```typescript
// ‚úÖ Bon
interface Props {
  onSubmit: (text: string) => Promise<void>;
  disabled?: boolean;
}

// ‚ùå √âviter
interface Props {
  onSubmit: any;
  disabled: boolean | undefined;
}
```

### Composants React

- **'use client'**: Obligatoire pour composants avec hooks/interactivit√©
- **Props explicites**: Interface d√©di√©e pour chaque composant
- **Pas de barrel exports**: Import direct des composants

```typescript
// ‚úÖ Bon
'use client';

interface VoiceRecorderProps {
  onTranscription: (text: string, audioUrl?: string) => void;
  onError?: (error: string) => void;
}

export function VoiceRecorder({ onTranscription, onError }: VoiceRecorderProps) {
  // ...
}

// ‚ùå √âviter
export default function VoiceRecorder(props: any) {
  // ...
}
```

### State Management

Utiliser Zustand pour le state global:

```typescript
// Pattern standard
import { create } from 'zustand';
import { persist } from 'zustand/middleware';

interface MyState {
  // State
  data: Data[];

  // Actions
  addData: (item: Data) => void;
  removeData: (id: string) => void;
}

export const useMyStore = create<MyState>()(
  persist(
    (set, get) => ({
      data: [],

      addData: (item) => {
        const { data } = get();
        set({ data: [...data, item] });
      },

      removeData: (id) => {
        const { data } = get();
        set({ data: data.filter(d => d.id !== id) });
      },
    }),
    {
      name: 'my-storage',
      // Ne persister que ce qui est n√©cessaire
      partialize: (state) => ({ data: state.data }),
    }
  )
);
```

## Modifications courantes

### Ajouter un nouveau sc√©nario

**1. √âditer** `public/scenarios/default-scenarios.json`:

```json
{
  "id": "mon-scenario",
  "title": "Mon Nouveau Sc√©nario",
  "description": "Description courte et claire",
  "category": "business",
  "difficulty": "intermediate",
  "aiRole": "R√¥le que l'IA doit jouer",
  "systemPrompt": "Instructions d√©taill√©es pour l'IA...",
  "suggestedDuration": 15,
  "focusAreas": ["focus1", "focus2"],
  "icon": "üéØ",
  "tags": ["tag1", "tag2"]
}
```

**2. Conventions:**
- `id`: kebab-case, unique, descriptif
- `systemPrompt`: Instructions claires sur le comportement de l'IA
- `focusAreas`: Ce sur quoi l'utilisateur doit se concentrer
- `icon`: Un seul emoji repr√©sentatif

### Modifier les prompts syst√®me

**Fichier:** `lib/ollama/prompts.ts`

**Prompts disponibles (tous sont maintenant des fonctions dynamiques):**

1. **`languageCoachPrompt(targetLanguage)`** - Conversation g√©n√©rale
   - Accepte 'en', 'fr', ou 'es'
   - G√©n√®re prompt adapt√© √† la langue cible
   - Doit rester encourageant et naturel
   - Ne PAS corriger directement dans la conversation
   - L'analyse se fait s√©par√©ment

2. **`feedbackAnalyzerPrompt(targetLanguage)`** - Analyse pour corrections
   - Adapte l'analyse √† la langue cible
   - Explications dans la langue native de l'utilisateur
   - Doit retourner du JSON valide
   - Format strict pour parsing
   - Prioriser les erreurs importantes

3. **`generateRolePlayPrompt(scenario, targetLanguage)`** - G√©n√®re prompt selon sc√©nario
   - Combine `languageCoachPrompt` + instructions du sc√©nario
   - Maintient le contexte du r√¥le
   - Adapt√© √† la langue cible

**Exemple de mapping langue:**
```typescript
const LANGUAGE_NAMES: Record<string, { learningName: string; nativeName: string }> = {
  en: { learningName: 'English', nativeName: 'French' },
  fr: { learningName: 'French', nativeName: 'English' },
  es: { learningName: 'Spanish', nativeName: 'English' },
};
```

**‚ö†Ô∏è Important:** Les prompts affectent directement la qualit√© de l'exp√©rience. Tester minutieusement apr√®s modification.

### Ajouter un nouveau type de feedback

**1. Mettre √† jour le type** dans `types/feedback.ts`:

```typescript
export type FeedbackType = 'grammar' | 'vocabulary' | 'style' | 'pronunciation'; // Ajouter ici
```

**2. Mettre √† jour** `feedbackAnalyzerPrompt` dans `lib/ollama/prompts.ts`:

```typescript
Focus on:
1. Grammar errors
2. Vocabulary issues
3. Style improvements
4. Pronunciation tips // Ajouter dans la liste
```

**3. Ajouter l'ic√¥ne** dans `components/feedback/correction-tooltip.tsx`:

```typescript
function getIconForType(type: string) {
  switch (type) {
    case 'grammar':
      return AlertCircle;
    case 'vocabulary':
      return AlertTriangle;
    case 'style':
      return Lightbulb;
    case 'pronunciation':
      return Volume2; // Nouveau
    default:
      return AlertCircle;
  }
}
```

### Changer le mod√®le Ollama

**1. Tirer le nouveau mod√®le:**
```bash
ollama pull nom-du-modele
```

**2. Mettre √† jour** `.env.local`:
```
OLLAMA_MODEL=nom-du-modele
```

**3. Consid√©rations:**
- Mod√®les plus petits = plus rapide, moins pr√©cis
- Mod√®les plus grands = plus lent, meilleur feedback
- V√©rifier RAM disponible (llama3.1:8b ‚âà 6GB)

### Modifier l'interface de chat

**Fichier principal:** `components/language-coach-chat.tsx`

Ce composant orchestre:
- `VoiceRecorder` - Capture audio
- `HighlightText` - Affichage avec feedback
- `FeedbackPanel` - R√©capitulatif
- `useChat` hook - Streaming AI
- TTS - Synth√®se vocale des r√©ponses
- Language selector - S√©lecteur de langue

**Pattern de donn√©es:**

```typescript
// Message utilisateur
{
  role: 'user',
  content: 'texte transcrit ou tap√©'
}

// R√©cup√©rer feedback avec langue cible
const response = await fetch('/api/feedback', {
  method: 'POST',
  body: JSON.stringify({
    text: userMessage,
    targetLanguage: targetLanguage  // 'en', 'fr', ou 'es'
  })
});

// Stocker feedback pour affichage
setCurrentFeedback({
  messageText: userMessage,  // Utiliser le texte au lieu de l'ID
  feedback: data.corrections,
  score: data.overallScore
});
```

### Support multilingue

Le syst√®me supporte trois langues : anglais, fran√ßais, espagnol.

**√âtat de la langue** (`lib/store/scenario-store.ts`):
```typescript
export type Language = 'en' | 'fr' | 'es';

const LANGUAGES: Record<Language, { name: string; flag: string; voiceLang: string }> = {
  en: { name: 'English', flag: 'üá¨üáß', voiceLang: 'en-US' },
  fr: { name: 'Fran√ßais', flag: 'üá´üá∑', voiceLang: 'fr-FR' },
  es: { name: 'Espa√±ol', flag: 'üá™üá∏', voiceLang: 'es-ES' },
};
```

**Prompts dynamiques** (`lib/ollama/prompts.ts`):
- `languageCoachPrompt(targetLanguage)` - G√©n√®re le prompt pour la langue cible
- `feedbackAnalyzerPrompt(targetLanguage)` - Adapte l'analyse √† la langue
- `generateRolePlayPrompt(scenario, targetLanguage)` - Combine sc√©nario et langue

**Hook useChat avec ID dynamique**:
```typescript
// ID change avec la langue pour conversations s√©par√©es
const { messages, sendMessage, status } = useChat({
  id: `coach-${targetLanguage}-${selectedScenario?.id || 'free'}`,
  transport: new DefaultChatTransport({
    api: '/api/ollama',
    body: {
      scenario: selectedScenario,
      targetLanguage: targetLanguage
    },
  }),
});
```

### Text-to-Speech (TTS)

**S√©lection intelligente des voix** (`components/language-coach-chat.tsx:63-91`):
```typescript
const selectBestVoice = (langCode: string) => {
  const voices = window.speechSynthesis.getVoices();
  const langVoices = voices.filter(v => v.lang.startsWith(langCode));

  // Priorit√©: Premium > Local > Default
  const premiumKeywords = ['premium', 'enhanced', 'neural', 'natural'];
  const premiumVoice = langVoices.find(v =>
    premiumKeywords.some(k => v.name.toLowerCase().includes(k))
  );

  return premiumVoice || langVoices.find(v => v.localService) || langVoices[0];
};
```

**Utilisation**:
```typescript
const utterance = new SpeechSynthesisUtterance(text);
utterance.lang = LANGUAGES[targetLanguage].voiceLang;
utterance.voice = selectBestVoice(langCode.split('-')[0]);
utterance.rate = 0.95;  // L√©g√®rement ralenti pour clart√©
window.speechSynthesis.speak(utterance);
```

**Pour voix de meilleure qualit√©**: Voir `docs/TTS-UPGRADE.md`

## API Routes

### POST /api/transcribe

**Input:** FormData avec fichier audio
**Output:** `{ transcription: string, timestamp: string }`

**Limites:**
- Max 10MB par fichier
- Formats: wav, webm, ogg, mp3, mp4
- Timeout: 30 secondes

**Gestion d'erreurs:**
- 400: Fichier manquant ou invalide
- 500: Erreur Whisper (service down, format non support√©)

### POST /api/ollama

**Input:**
```json
{
  "messages": [
    { "role": "user", "content": "Hello" }
  ],
  "scenario": { /* Scenario object */ },
  "targetLanguage": "en"  // 'en', 'fr', ou 'es'
}
```

**Output:** Stream de texte via `toUIMessageStreamResponse()`

**Configuration:**
- Temperature: 0.7 (√©quilibre cr√©ativit√©/coh√©rence)
- MaxTokens: Non sp√©cifi√© (laiss√© au mod√®le)
- Contexte: 10 derniers messages max
- Prompt syst√®me adapt√© √† la langue cible

### POST /api/feedback

**Input:**
```json
{
  "text": "texte √† analyser",
  "context": "scenario title (optionnel)",
  "userLevel": "intermediate",
  "targetLanguage": "en"  // 'en', 'fr', ou 'es'
}
```

**Output:**
```json
{
  "original": "texte original",
  "corrections": [
    {
      "id": "abc123",
      "type": "grammar",
      "severity": "error",
      "original": "I go yesterday",
      "suggestion": "I went yesterday",
      "explanation": "Use past tense for past actions",
      "startIndex": 0,
      "endIndex": 13
    }
  ],
  "overallScore": 85,
  "summary": "Good! A few improvements suggested."
}
```

**Note:** Les explications sont fournies dans la langue native de l'utilisateur (d√©finie par la langue cible).

## Patterns importants

### Gestion audio avec RecordRTC

```typescript
// Cr√©er recorder
const recorder = new RecordRTC(stream, {
  type: 'audio',
  mimeType: 'audio/webm',
  recorderType: RecordRTC.StereoAudioRecorder,
  numberOfAudioChannels: 1,
  desiredSampRate: 16000, // Whisper pr√©f√®re 16kHz
});

// D√©marrer
recorder.startRecording();

// Arr√™ter et r√©cup√©rer
recorder.stopRecording(() => {
  const blob = recorder.getBlob();
  // Envoyer √† /api/transcribe
});

// ‚ö†Ô∏è Important: Toujours nettoyer
stream.getTracks().forEach(track => track.stop());
```

### Streaming avec Vercel AI SDK

```typescript
// Server-side (API route)
import { streamText } from 'ai';
import { languageModel } from '@/lib/ollama/client';

const result = streamText({
  model: languageModel,
  messages: [...messages],
  temperature: 0.7,
});

return result.toDataStreamResponse();

// Client-side
import { useChat } from 'ai/react';

const { messages, append, isLoading } = useChat({
  api: '/api/ollama',
  onError: (error) => {
    toast.error('Error: ' + error.message);
  },
});
```

### Highlighting avec feedback

```typescript
// Cr√©er des segments de texte
function createTextSegments(text: string, feedback: Feedback[]) {
  const sorted = [...feedback].sort((a, b) => a.startIndex - b.startIndex);

  const segments = [];
  let currentIndex = 0;

  for (const fb of sorted) {
    // Texte avant
    if (currentIndex < fb.startIndex) {
      segments.push({ text: text.slice(currentIndex, fb.startIndex) });
    }

    // Texte avec feedback
    segments.push({
      text: text.slice(fb.startIndex, fb.endIndex),
      feedback: fb
    });

    currentIndex = fb.endIndex;
  }

  // Texte restant
  if (currentIndex < text.length) {
    segments.push({ text: text.slice(currentIndex) });
  }

  return segments;
}
```

## Pi√®ges √† √©viter

### ‚ùå Ne pas faire

1. **Modifier le contexte de conversation sans limite**
   - Garder max 10 messages pour performance
   - Plus = plus lent et plus de RAM

2. **Oublier de nettoyer les ressources audio**
   ```typescript
   // ‚ùå Mauvais
   recorder.stopRecording(() => {
     const blob = recorder.getBlob();
   });

   // ‚úÖ Bon
   recorder.stopRecording(() => {
     const blob = recorder.getBlob();
     stream.getTracks().forEach(track => track.stop());
   });
   ```

3. **Parser JSON sans try/catch**
   ```typescript
   // ‚ùå Mauvais
   const corrections = JSON.parse(result.text);

   // ‚úÖ Bon
   try {
     const corrections = JSON.parse(result.text);
   } catch (error) {
     console.error('Failed to parse:', result.text);
     return [];
   }
   ```

4. **Ignorer les erreurs Ollama/Whisper**
   - Toujours v√©rifier que les services sont en ligne
   - Fournir des messages d'erreur clairs pour l'utilisateur

5. **Envoyer tout le feedback √† l'utilisateur**
   - Prioriser: errors > warnings > suggestions
   - Max 5 corrections par message id√©alement

### ‚úÖ Bonnes pratiques

1. **Tester les services au d√©marrage**
   ```typescript
   useEffect(() => {
     const checkServices = async () => {
       const ollamaOk = await fetch('/api/ollama').then(r => r.ok);
       const whisperOk = await fetch('/api/transcribe').then(r => r.ok);

       if (!ollamaOk) toast.error('Ollama not available');
       if (!whisperOk) toast.error('Whisper not available');
     };

     checkServices();
   }, []);
   ```

2. **Debounce des appels API co√ªteux**
   - Pas n√©cessaire pour le chat (streaming)
   - Utile si on ajoute de l'auto-feedback pendant la frappe

3. **Validation des entr√©es utilisateur**
   ```typescript
   // Avant d'envoyer √† l'API
   if (text.length > 5000) {
     toast.error('Text too long (max 5000 characters)');
     return;
   }
   ```

4. **Gestion des permissions microphone**
   ```typescript
   try {
     const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
   } catch (error) {
     if (error.name === 'NotAllowedError') {
       toast.error('Microphone access denied');
     } else if (error.name === 'NotFoundError') {
       toast.error('No microphone found');
     }
   }
   ```

## Performance

### Optimisations actuelles

1. **Contexte limit√©**: 10 messages max envoy√©s √† Ollama
2. **Token limit**: 1000 tokens max par r√©ponse
3. **Streaming**: R√©ponses stream√©es pour UX responsive
4. **Cleanup**: Fichiers audio temporaires supprim√©s imm√©diatement
5. **Persistence s√©lective**: Seules les conversations sauv√©es en localStorage

### M√©triques √† surveiller

- **Temps de transcription**: < 5s pour 30s d'audio
- **Temps premi√®re r√©ponse**: < 2s apr√®s envoi message
- **M√©moire**: < 8GB utilis√©s au total
- **Latence streaming**: Tokens doivent appara√Ætre progressivement

### Si performance d√©grad√©e

1. R√©duire le contexte (de 10 √† 5 messages)
2. Utiliser un mod√®le plus petit (llama3.1:7b)
3. R√©duire maxTokens (de 1000 √† 500)
4. Vider le cache Ollama: `ollama rm model && ollama pull model`

## Testing

### Test manuel recommand√©

**Workflow de test complet:**

1. **Services**
   - [ ] Ollama running: `curl http://localhost:11434/api/tags`
   - [ ] Whisper accessible: `whisper --version`

2. **Page /coach**
   - [ ] Affichage correct du message de bienvenue
   - [ ] Bouton micro visible et cliquable
   - [ ] Input texte fonctionnel

3. **Voice recording**
   - [ ] Click micro ‚Üí animation recording
   - [ ] Permission microphone demand√©e
   - [ ] Recording ‚Üí Transcribing ‚Üí Message appara√Æt
   - [ ] Audio transcrit correctement

4. **AI response**
   - [ ] R√©ponse appara√Æt progressivement (streaming)
   - [ ] Ton encourageant et naturel
   - [ ] Contextuel (si sc√©nario s√©lectionn√©)

5. **Feedback**
   - [ ] Mots/phrases surlign√©s en couleur
   - [ ] Click ‚Üí tooltip avec d√©tails
   - [ ] Panneau r√©capitulatif affiche score

6. **Page /scenarios**
   - [ ] Cards affich√©es correctement
   - [ ] S√©lection ‚Üí redirection vers /coach
   - [ ] Sc√©nario actif affich√© dans header

### Tests unitaires (√† ajouter)

```typescript
// Exemple: tests pour createTextSegments
describe('createTextSegments', () => {
  it('should create segments with feedback', () => {
    const text = "I go to store";
    const feedback = [{
      startIndex: 2,
      endIndex: 4,
      type: 'grammar',
      // ...
    }];

    const segments = createTextSegments(text, feedback);

    expect(segments).toHaveLength(3);
    expect(segments[1].feedback).toBeDefined();
  });
});
```

## D√©ploiement

### Consid√©rations

Le projet est con√ßu pour fonctionner **localement uniquement**. Un d√©ploiement classique ne fonctionnera pas car:

- Ollama doit tourner localement ou sur un serveur accessible
- Whisper n√©cessite acc√®s aux binaires syst√®me
- Microphone n√©cessite HTTPS ou localhost

### Options de d√©ploiement

**Option 1: Docker local**
- Packager Ollama + Whisper + App dans un container
- L'utilisateur lance le container localement
- Acc√®s via `localhost:3000`

**Option 2: Electron app**
- Distribuer comme app desktop
- Inclure Ollama et Whisper
- Installation simplifi√©e pour utilisateurs

**Option 3: Cloud hybride** (‚ö†Ô∏è perd privacy-first)
- Frontend sur Vercel
- Ollama sur serveur GPU d√©di√©
- Whisper via API (OpenAI, AssemblyAI...)

## D√©pendances critiques

### Versions √† maintenir

```json
{
  "next": "15.3.0-canary.31",         // App Router stable
  "react": "19.0.0-rc",               // Concurrent features
  "ai": "5.0.26",                     // Vercel AI SDK
  "ollama-ai-provider-v2": "1.5.5",  // Provider Ollama (official)
  "zustand": "5.0.8",                 // State management
  "recordrtc": "5.6.2"                // Audio recording
}
```

### Migration √† surveiller

- **React 19 RC ‚Üí Stable**: Mettre √† jour quand disponible
- **Next.js**: Suivre les canary pour fixes App Router
- **Vercel AI SDK**: API en √©volution rapide

## Support et ressources

### Documentation externe

- [Vercel AI SDK](https://sdk.vercel.ai/)
- [Ollama](https://ollama.com/)
- [Whisper](https://github.com/openai/whisper)
- [RecordRTC](https://recordrtc.org/)
- [Zustand](https://zustand.docs.pmnd.rs/)

### Fichiers cl√©s √† conna√Ætre

1. **`lib/ollama/prompts.ts`** - Tous les prompts syst√®me
2. **`components/language-coach-chat.tsx`** - Orchestration principale
3. **`app/api/feedback/route.ts`** - Logique d'analyse
4. **`types/index.ts`** - Toutes les interfaces TypeScript

### Debugging

**Activer les logs d√©taill√©s:**

```typescript
// Dans lib/ollama/client.ts
export const languageModel = ollamaProvider(modelName, {
  // Ajouter pour debug
  fetch: async (url, options) => {
    console.log('Ollama request:', url, options);
    const response = await fetch(url, options);
    console.log('Ollama response:', response);
    return response;
  }
});
```

**Logs Whisper:**
- Check `console.error` dans `/api/transcribe`
- Test manuel: `whisper audio.wav --model small`

**Logs RecordRTC:**
```typescript
RecordRTC.prototype.debug = true; // Active les logs internes
```

## Nouvelles fonctionnalit√©s (2024)

### Support multilingue (EN/FR/ES)
- S√©lecteur de langue dans l'interface
- Prompts dynamiques adapt√©s √† chaque langue
- Conversations s√©par√©es par langue
- Feedback adapt√© √† la langue cible

### Text-to-Speech
- Synth√®se vocale des r√©ponses de l'IA
- S√©lection automatique des meilleures voix
- Support vocal pour les 3 langues
- Toggle pour activer/d√©sactiver

### Am√©lioration UX
- Modal de s√©lection de sc√©narios
- Panel de feedback fixe √† droite
- Transcription affich√©e pour r√©vision avant envoi
- Feedback tracking am√©lior√© (par texte au lieu d'ID)

## Documentation additionnelle

- **TTS-UPGRADE.md** - Guide pour am√©liorer la qualit√© des voix TTS
- **SETUP-DB.md** - Configuration PostgreSQL et Redis

## Conclusion

Ce projet privil√©gie:
- üîí **Privacy** - Tout local, rien ne quitte la machine
- üåç **Multilingue** - Support natif EN/FR/ES
- üí™ **Encouragement** - Feedback constructif, jamais d√©courageant
- üéØ **Focus** - 2-3 corrections importantes plut√¥t que tout
- ‚ö° **Performance** - Optimis√© pour machines modernes

Avant toute modification majeure, consid√©rer l'impact sur ces principes fondamentaux.

---

**Questions?** Consultez le README.md pour le setup ou ouvrez une issue.
